# -*- coding: utf-8 -*-
"""Team_Cropruster.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nUpkvEUmUMNg2-F0l2xk2y2AlVG1btCH
"""

#Basic DS imports
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from os import path
current_directory = path.dirname(path.abspath(__file__))
path = current_directory + "..\\data\\imdb-videogames.csv"

df = pd.read_csv(path)
df.head()

sns.scatterplot(data=df, x="year", y="rating")

df = df.drop_duplicates()
df = df.drop(columns=['Unnamed: 0', 'name', 'year', 'url', 'plot'])

df = df.dropna()

df['votes'] = df['votes'].str.replace(',', '').astype(int)
df = df.loc[df['votes']>=100,:]
df.reset_index(inplace=True, drop=True)
df

"""END OF PART 2"""

df['Action'] = df['Action'].map({True: 1, False: 0})
df['Adventure'] = df['Adventure'].map({True: 1, False: 0})
df['Comedy'] = df['Comedy'].map({True: 1, False: 0})
df['Crime'] = df['Crime'].map({True: 1, False: 0})
df['Family'] = df['Family'].map({True: 1, False: 0})
df['Fantasy'] = df['Fantasy'].map({True: 1, False: 0})
df['Mystery'] = df['Mystery'].map({True: 1, False: 0})
df['Sci-Fi'] = df['Sci-Fi'].map({True: 1, False: 0})
df['Thriller'] = df['Thriller'].map({True: 1, False: 0})

df

df = df.drop(columns='votes')
df

df['certificate'].value_counts()

df = df.drop(df[df['certificate'] == 'K-A'].index)
df = df.drop(df[df['certificate'] == 'Unrated'].index)
df = df.drop(df[df['certificate'] == 'Not Rated'].index)
df = df.drop(df[df['certificate'] == 'GA'].index)
df = df.drop(df[df['certificate'] == 'AO'].index)
df = df.drop(df[df['certificate'] == 'PG-13'].index)
df = df.drop(df[df['certificate'] == 'MA-13'].index)
df = df.drop(df[df['certificate'] == 'MA-17'].index)
df = df.drop(df[df['certificate'] == 'TV-14'].index)
df = df.drop(df[df['certificate'] == 'PG'].index)
df.reset_index(inplace=True, drop=True)

df

df = pd.get_dummies(df)
df

df.iloc[:,2:-1].sum().plot(kind='bar', ylabel='Number of Games', xlabel='Genre')

corr = df.corr()
corr.style.background_gradient(cmap = 'coolwarm')

#df.reset_index(drop=True, inplace=True)
#df = df.dropna()
X = df.drop(columns = 'rating')
y = df['rating']

features = [X.columns]

y = y.round()
y = y.astype(int)
X

# from sklearn.preprocessing import StandardScaler
# scaler = StandardScaler()
# scaledDf = scaler.fit_transform(X)
# scaledDf = pd.DataFrame(data = scaledDf, columns = features)
# scaledDf

from sklearn.decomposition import PCA

pca = PCA()
X = pca.fit(X).transform(X)

pd.DataFrame(X)

import matplotlib.pyplot as plt
PC_values = np.arange(pca.n_components_)+1

plt.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')

plt.title('Scree Plot')
plt.xlabel('Principal Component')
plt.ylabel('Variance Explained')
plt.show()

X = pd.DataFrame(X[:,0:2], columns = ['First Principal Component', 'Second Principal Component'])
sns.scatterplot(x = X['First Principal Component'], y = X['Second Principal Component'], hue = y, alpha = .75, edgecolor="black", palette = 'viridis')

########Do this before or after below code window.  Look and see which one is for testing accuracy of the model and which one is for actuall model training proability

#Code for splitting the data

#Use below to find accuracy for each model (?? Idk if this makes sense ?? )
##Train all models on split data
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score

knn2 = KNeighborsClassifier(n_neighbors= 2)
knn5 = KNeighborsClassifier(n_neighbors= 5)
knn8 = KNeighborsClassifier(n_neighbors= 8)
tree = DecisionTreeClassifier()
tree3 = DecisionTreeClassifier(max_depth= 3)
tree5 = DecisionTreeClassifier(max_depth= 5)
tree8 = DecisionTreeClassifier(max_depth= 8)
log_reg = LogisticRegression()
svc = SVC()
classifiers = [knn2, knn5, knn8, tree, tree3, tree5, tree8, log_reg, svc]

accuracy_dict = {}

for classifier in classifiers:
  accuracy_dict[classifier] = []

print(accuracy_dict)

# np.ones((25, 1)), np.hstack(([0] * 20, [1] * 5))
skf = StratifiedKFold(n_splits=2)
for train_index, test_index in skf.split(X, y):

  X_train, X_test = X.iloc[train_index], X.iloc[test_index]
  y_train, y_test = y.iloc[train_index], y.iloc[test_index]

  for model in classifiers:
    # Fit the model on the training data
    model.fit(X_train, y_train)

    # Make predictions on the test data
    y_pred = model.predict(X_test)

    # Calculate and store the accuracy score
    accuracy = accuracy_score(y_test, y_pred)
    accuracy_dict[model].append(accuracy)


# average_accuracy = sum(accuracy_scores) / n_splits
# print(f'Average Accuracy: {average_accuracy}')
print(accuracy_dict)

###EX after training. knn2.predict_proba

max_acc = -1
max_model = 0
for c in accuracy_dict:
  print(c)
  acc = sum(accuracy_dict[c])/len(accuracy_dict[c])
  print(acc)
  if max_acc < acc:
    max_acc = acc
    max_model = c

print('Max model is {} with accuracy of {}'.format(max_model, max_acc))